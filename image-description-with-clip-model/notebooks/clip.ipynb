{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c64a8ac6",
   "metadata": {},
   "source": [
    "### **Import Libraries and Modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48a55590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf23874",
   "metadata": {},
   "source": [
    "### **Positional Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d6aee8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "  def __init__(self, width, max_seq_length):\n",
    "    super().__init__()\n",
    "\n",
    "    # Creating positional encoding\n",
    "    pe = torch.zeros(max_seq_length, width)\n",
    "\n",
    "    for pos in range(max_seq_length):\n",
    "      for i in range(width):\n",
    "        if i % 2 == 0:\n",
    "          pe[pos][i] = np.sin(pos/(10000 ** (i/width)))\n",
    "        else:\n",
    "          pe[pos][i] = np.cos(pos/(10000 ** ((i-1)/width)))\n",
    "\n",
    "    self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Add positional encoding to embeddings\n",
    "    x = x + self.pe\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6712c6fd",
   "metadata": {},
   "source": [
    "### **Multi-Head Attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56f58259",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "  def __init__(self, width, head_size):\n",
    "    super().__init__()\n",
    "    self.head_size = head_size\n",
    "\n",
    "    self.query = nn.Linear(width, head_size)\n",
    "    self.key = nn.Linear(width, head_size)\n",
    "    self.value = nn.Linear(width, head_size)\n",
    "\n",
    "  def forward(self, x, mask=None):\n",
    "    # Obtaining Queries, Keys, and Values\n",
    "    Q = self.query(x)\n",
    "    K = self.key(x)\n",
    "    V = self.value(x)\n",
    "\n",
    "    # Dot Product of Queries and Keys\n",
    "    attention = Q @ K.transpose(-2,-1)\n",
    "\n",
    "    # Scaling\n",
    "    attention = attention / (self.head_size ** 0.5)\n",
    "\n",
    "    # Applying Attention Mask\n",
    "    if mask is not None:\n",
    "        attention = attention.masked_fill(mask == 0, float(\"-inf\"))\n",
    "\n",
    "    attention = torch.softmax(attention, dim=-1)\n",
    "\n",
    "    attention = attention @ V\n",
    "\n",
    "    return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "522cf500",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, width, n_heads):\n",
    "    super().__init__()\n",
    "    self.head_size = width // n_heads\n",
    "\n",
    "    self.W_o = nn.Linear(width, width)\n",
    "\n",
    "    self.heads = nn.ModuleList([AttentionHead(width, self.head_size) for _ in range(n_heads)])\n",
    "\n",
    "  def forward(self, x, mask=None):\n",
    "    # Combine attention heads\n",
    "    out = torch.cat([head(x, mask=mask) for head in self.heads], dim=-1)\n",
    "\n",
    "    out = self.W_o(out)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31579d11",
   "metadata": {},
   "source": [
    "### **Transformer Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7c2113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, width, n_heads, r_mlp=4):\n",
    "        super().__init__()\n",
    "        self.width = width\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        # Sub-Layer 1 Normalization\n",
    "        self.ln1 = nn.LayerNorm(width)\n",
    "\n",
    "        # Multi-Head Attention\n",
    "        self.mha = MultiHeadAttention(width, n_heads)\n",
    "\n",
    "        # Sub-Layer 2 Normalization\n",
    "        self.ln2 = nn.LayerNorm(width)\n",
    "\n",
    "        # Multilayer Perception\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.width, self.width*r_mlp),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.width*r_mlp, self.width)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Residual Connection After Sub-Layer 1\n",
    "        x = x + self.mha(self.ln1(x), mask=mask)\n",
    "\n",
    "        # Residual Connection After Sub-Layer 2\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85097bad",
   "metadata": {},
   "source": [
    "### **Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78f84e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text, encode=True, mask=None, max_seq_length=32):\n",
    "    if encode:\n",
    "        out = chr(2) + text + chr(3) # Adding SOT and EOT tokens\n",
    "        out = out + \"\".join([chr(0) for _ in range(max_seq_length-len(out))]) # Adding Padding\n",
    "        out = torch.IntTensor(list(out.encode(\"utf-8\"))) # Encoding Text\n",
    "        mask = torch.ones(len(out.nonzero()))\n",
    "        mask = torch.cat((mask,torch.zeros(max_seq_length-len(mask)))).type(torch.IntTensor)\n",
    "    else:\n",
    "        out = [chr(x) for x in text[1:len(mask.nonzero())-1]]\n",
    "        out = \"\".join(out)\n",
    "        mask = None\n",
    "\n",
    "    return out, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdefb444",
   "metadata": {},
   "source": [
    "### **Text Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6d64dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, width, max_seq_length, n_heads, n_layers, emb_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_seq_length = max_seq_length  # Maximum length of input sequence\n",
    "\n",
    "        self.encoder_embedding = nn.Embedding(vocab_size, width) # Embedding Table\n",
    "\n",
    "        self.positional_embedding = PositionalEmbedding(width, max_seq_length)\n",
    "\n",
    "        self.encoder = nn.ModuleList([TransformerEncoder(width,n_heads) for _ in range(n_layers)])\n",
    "\n",
    "        # learned proj of image to embed\n",
    "        self.projection = nn.Parameter(torch.randn(width, emb_dim))\n",
    "\n",
    "    def forward(self, text, mask=None):\n",
    "        # Text Embedding\n",
    "        x = self.encoder_embedding(text)\n",
    "\n",
    "        # Positional Embedding\n",
    "        x = self.positional_embedding(x)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        for encoder_layer in self.encoder:\n",
    "            x = encoder_layer(x, mask=mask)\n",
    "\n",
    "        # Takes features from the EOT Embedding\n",
    "        x = x[torch.arange(text.shape[0]),torch.sub(torch.sum(mask[:,0],dim=1),1)]\n",
    "\n",
    "        # joint multimodal embedding\n",
    "        if self.projection is not None:\n",
    "            x = x @ self.projection\n",
    "\n",
    "        x = x / torch.norm(x, dim=-1, keepdim=True)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384975b9",
   "metadata": {},
   "source": [
    "### **Image Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dfe3db1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, width, img_size, patch_size, n_channels, n_layers, n_heads, emb_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        assert img_size[0] % patch_size[0] == 0 and img_size[1] % patch_size[1] == 0, \"img_size dimensions must be divisible by patch_size dimensions\"\n",
    "        assert width % n_heads == 0, \"width must be divisible by n_heads\"\n",
    "\n",
    "        self.n_patches = (img_size[0] * img_size[1]) // (patch_size[0] * patch_size[1])\n",
    "\n",
    "        self.max_seq_length = self.n_patches + 1\n",
    "\n",
    "        # Patch Embedding\n",
    "        self.linear_project = nn.Conv2d(n_channels, width, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "        # Classification Token\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, width))\n",
    "\n",
    "        self.positional_embedding = PositionalEmbedding(width,self.max_seq_length)\n",
    "\n",
    "        self.encoder = nn.ModuleList([TransformerEncoder(width,n_heads) for _ in range(n_layers)])\n",
    "\n",
    "        # learned proj of image to embed\n",
    "        self.projection = nn.Parameter(torch.randn(width, emb_dim))\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        # Patch Embedding\n",
    "        x = self.linear_project(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "\n",
    "        # Positional Embedding\n",
    "        x = torch.cat((self.cls_token.expand(x.size()[0], -1, -1),x), dim=1)\n",
    "        x = self.positional_embedding(x)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        for encoder_layer in self.encoder:\n",
    "            x = encoder_layer(x)\n",
    "\n",
    "        # Takes Class Tokens\n",
    "        x = x[:, 0, :]\n",
    "\n",
    "        # joint multimodal embedding\n",
    "        if self.projection is not None:\n",
    "            x = x @ self.projection\n",
    "\n",
    "        x = x / torch.norm(x, dim=-1, keepdim=True)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6163bd2",
   "metadata": {},
   "source": [
    "### **CLIP Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b94fef21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP(nn.Module):\n",
    "    def __init__(self, emb_dim, vit_width, img_size, patch_size, n_channels, vit_layers, vit_heads, vocab_size, text_width, max_seq_length, text_heads, text_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_encoder = ImageEncoder(vit_width, img_size, patch_size, n_channels, vit_layers, vit_heads, emb_dim)\n",
    "\n",
    "        self.text_encoder = TextEncoder(vocab_size, text_width, max_seq_length, text_heads, text_layers, emb_dim)\n",
    "\n",
    "        self.temperature = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "    def forward(self,image,text, mask=None):\n",
    "        I_e = self.image_encoder(image)\n",
    "        T_e = self.text_encoder(text, mask=mask)\n",
    "\n",
    "        # scaled pairwise cosine similarities [n, n]\n",
    "        logits = (I_e @ T_e.transpose(-2,-1)) * torch.exp(self.temperature)\n",
    "\n",
    "        # symmetric loss function\n",
    "        labels = torch.arange(logits.shape[0]).to(self.device)\n",
    "\n",
    "        loss_i = nn.functional.cross_entropy(logits.transpose(-2,-1), labels)\n",
    "        loss_t = nn.functional.cross_entropy(logits, labels)\n",
    "\n",
    "        loss = (loss_i + loss_t) / 2\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa3115b",
   "metadata": {},
   "source": [
    "### **Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2bce2736",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNIST(Dataset):\n",
    "    def __init__(self, train=True):\n",
    "        self.dataset = load_dataset(\"fashion_mnist\")\n",
    "\n",
    "        self.transform = T.ToTensor()\n",
    "\n",
    "        if train:\n",
    "            self.split = \"train\"\n",
    "        else:\n",
    "            self.split = \"test\"\n",
    "\n",
    "\n",
    "        self.captions = {0: \"An image of a t-shirt/top\",\n",
    "                        1: \"An image of trousers\",\n",
    "                        2: \"An image of a pullover\",\n",
    "                        3: \"An image of a dress\",\n",
    "                        4: \"An image of a coat\",\n",
    "                        5: \"An image of a sandal\",\n",
    "                        6: \"An image of a shirt\",\n",
    "                        7: \"An image of a sneaker\",\n",
    "                        8: \"An image of a bag\",\n",
    "                        9: \"An image of an ankle boot\"}\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.num_rows[self.split]\n",
    "\n",
    "    def __getitem__(self,i):\n",
    "        img = self.dataset[self.split][i][\"image\"]\n",
    "        img = self.transform(img)\n",
    "\n",
    "        cap, mask = tokenizer(self.captions[self.dataset[self.split][i][\"label\"]])\n",
    "\n",
    "        mask = mask.repeat(len(mask),1)\n",
    "\n",
    "        return {\"image\": img, \"caption\": cap, \"mask\": mask}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb59a5c2",
   "metadata": {},
   "source": [
    "### **Training Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fab917c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 32\n",
    "vit_width = 9\n",
    "img_size = (28,28)\n",
    "patch_size = (14,14)\n",
    "n_channels = 1\n",
    "vit_layers = 3\n",
    "vit_heads = 3\n",
    "vocab_size = 256\n",
    "text_width = 32\n",
    "max_seq_length = 32\n",
    "text_heads = 8\n",
    "text_layers = 4\n",
    "lr = 1e-3\n",
    "epochs = 10 \n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166b6c2f",
   "metadata": {},
   "source": [
    "### **Loading Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dedcc8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = FashionMNIST(train = True)\n",
    "test_set = FashionMNIST(train = False)\n",
    "\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_set, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f43e60f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "torch.Size([32])\n",
      "torch.Size([32, 32])\n",
      "tensor([  2,  65, 110,  32, 105, 109,  97, 103, 101,  32, 111, 102,  32,  97,\n",
      "        110,  32,  97, 110, 107, 108, 101,  32,  98, 111, 111, 116,   3,   0,\n",
      "          0,   0,   0,   0], dtype=torch.int32)\n",
      "tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "print(train_set[0][\"image\"].shape)\n",
    "print(train_set[0][\"caption\"].shape)\n",
    "print(train_set[0][\"mask\"].shape)\n",
    "print(train_set[0][\"caption\"])\n",
    "print(train_set[0][\"mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de875004",
   "metadata": {},
   "source": [
    "### **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e4ba78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda (NVIDIA GeForce RTX 3050 Ti Laptop GPU)\n",
      "Epoch [1/10], Batch Loss: 3.138\n",
      "Model Saved.\n",
      "Epoch [2/10], Batch Loss: 2.803\n",
      "Model Saved.\n",
      "Epoch [3/10], Batch Loss: 2.678\n",
      "Model Saved.\n",
      "Epoch [4/10], Batch Loss: 2.864\n",
      "Epoch [5/10], Batch Loss: 2.800\n",
      "Epoch [6/10], Batch Loss: 2.790\n",
      "Epoch [7/10], Batch Loss: 2.594\n",
      "Model Saved.\n",
      "Epoch [8/10], Batch Loss: 2.719\n",
      "Epoch [9/10], Batch Loss: 2.673\n",
      "Epoch [10/10], Batch Loss: 2.625\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")\n",
    "\n",
    "model = CLIP(emb_dim, vit_width, img_size, patch_size, n_channels, vit_layers, vit_heads, vocab_size, text_width, max_seq_length, text_heads, text_layers).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "best_loss = np.inf\n",
    "for epoch in range(epochs):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        img, cap, mask = data[\"image\"].to(device), data[\"caption\"].to(device), data[\"mask\"].to(device)\n",
    "        loss = model(img,cap,mask)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Saves model if it performed better than the previous best\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Batch Loss: {loss.item():.3f}\")\n",
    "    if loss.item() <= best_loss:\n",
    "        best_loss = loss.item()\n",
    "        torch.save(model.state_dict(), \"../src/clip.pt\")\n",
    "        print(\"Model Saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ad08a7",
   "metadata": {},
   "source": [
    "### **Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "396eecf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16405/437209580.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"../src/clip.pt\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Accuracy: 84 %\n"
     ]
    }
   ],
   "source": [
    "# Loading Best Model\n",
    "model = CLIP(emb_dim, vit_width, img_size, patch_size, n_channels, vit_layers, vit_heads, vocab_size, text_width, max_seq_length, text_heads, text_layers).to(device)\n",
    "model.load_state_dict(torch.load(\"../src/clip.pt\", map_location=device))\n",
    "\n",
    "# Getting dataset captions to compare images to\n",
    "text = torch.stack([tokenizer(x)[0] for x in test_set.captions.values()]).to(device)\n",
    "mask = torch.stack([tokenizer(x)[1] for x in test_set.captions.values()])\n",
    "mask = mask.repeat(1,len(mask[0])).reshape(len(mask),len(mask[0]),len(mask[0])).to(device)\n",
    "\n",
    "correct, total = 0,0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data[\"image\"].to(device), data[\"caption\"].to(device)\n",
    "        image_features = model.image_encoder(images)\n",
    "        text_features = model.text_encoder(text, mask=mask)\n",
    "\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "        similarity = (100.0 * (image_features @ text_features.T)).softmax(dim=-1)\n",
    "        _, indices = torch.max(similarity,1)\n",
    "        pred = torch.stack([tokenizer(test_set.captions[int(i)])[0] for i in indices]).to(device)\n",
    "        correct += int(sum(torch.sum((pred==labels),dim=1)//len(pred[0])))\n",
    "        total += len(labels)\n",
    "\n",
    "print(f'\\nModel Accuracy: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbf61c7",
   "metadata": {},
   "source": [
    "### **Zero-Shot Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ce05038a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16405/577610390.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"../src/clip.pt\", map_location=device))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top predictions:\n",
      "\n",
      "     t-shirt/top: 85.21%\n",
      "           shirt: 13.21%\n",
      "          sandal: 1.45%\n",
      "        pullover: 0.13%\n",
      "            coat: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Loading Best Model\n",
    "model = CLIP(emb_dim, vit_width, img_size, patch_size, n_channels, vit_layers, vit_heads, vocab_size, text_width, max_seq_length, text_heads, text_layers).to(device)\n",
    "model.load_state_dict(torch.load(\"../src/clip.pt\", map_location=device))\n",
    "\n",
    "\n",
    "# Captions to compare images to\n",
    "class_names =[\"t-shirt/top\",\n",
    "                        \"trousers\",\n",
    "                        \"pullover\",\n",
    "                        \"dress\",\n",
    "                        \"coat\",\n",
    "                        \"sandal\",\n",
    "                        \"shirt\",\n",
    "                        \"sneaker\",\n",
    "                        \"bag\",\n",
    "                        \"ankle boot\"]\n",
    "\n",
    "# class_names =[\"An image of a t-shirt/top\",\n",
    "#                         \"An image of trousers\",\n",
    "#                         \"An image of a pullover\",\n",
    "#                         \"An image of a dress\",\n",
    "#                         \"An image of a coat\",\n",
    "#                         \"An image of a sandal\",\n",
    "#                         \"An image of a shirt\",\n",
    "#                         \"An image of a sneaker\",\n",
    "#                         \"An image of a bag\",\n",
    "#                         \"An image of an ankle boot\"]\n",
    "\n",
    "text = torch.stack([tokenizer(x)[0] for x in class_names]).to(device)\n",
    "mask = torch.stack([tokenizer(x)[1] for x in class_names])\n",
    "mask = mask.repeat(1,len(mask[0])).reshape(len(mask),len(mask[0]),len(mask[0])).to(device)\n",
    "\n",
    "idx = 1000\n",
    "\n",
    "img = test_set[idx][\"image\"][None,:]\n",
    "plt.imshow(  img[0].permute(1, 2, 0)  ,cmap=\"gray\")\n",
    "plt.title(tokenizer(test_set[idx][\"caption\"], encode=False, mask=test_set[idx][\"mask\"][0])[0])\n",
    "plt.show()\n",
    "img = img.to(device)\n",
    "with torch.no_grad():\n",
    "  image_features = model.image_encoder(img)\n",
    "  text_features = model.text_encoder(text, mask=mask)\n",
    "\n",
    "\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "values, indices = similarity[0].topk(5)\n",
    "\n",
    "# Print the result\n",
    "print(\"\\nTop predictions:\\n\")\n",
    "for value, index in zip(values, indices):\n",
    "    print(f\"{class_names[int(index)]:>16s}: {100 * value.item():.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
